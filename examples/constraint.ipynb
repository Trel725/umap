{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### constraints.py demo\n",
    "This runs through several short demos of how to slap\n",
    "constraints onto the lo-dimensional embedding of UMAP.\n",
    "\n",
    "Dataset-independent constraints can be supplied to the constructor;\n",
    "dataset-dependent ones, to the 'fit' or 'fit_transform' function.\n",
    "The latter have a first argument that is always the index of the point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "import numpy as np\n",
    "import umap\n",
    "\n",
    "iris = load_iris()\n",
    "umapper0 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12345, min_dist=0.001,\n",
    "    init=\"random\", n_epochs=1,\n",
    ")\n",
    "print(\"Generating an initial embedding...\")\n",
    "emb0 = umapper0.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo 1- and 2-d pin mask, data_constrain=array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Pinning embeddings of pts 13 and 14 to [-5,0] and [5,0]\")\n",
    "# pin embeddings of two data (13 and 14) to left and right of origin\n",
    "pin_mask = np.ones_like(emb0)\n",
    "pin_mask[13] = 0.0\n",
    "pin_mask[14] = 0.0\n",
    "emb0[13] = [-5.0, 0]\n",
    "emb0[14] = [+5.0, 0]\n",
    "print(\"Specify 'init' embedding for umapper2\")\n",
    "umapper1 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12346, min_dist=0.001,\n",
    "    init=emb0, n_epochs=2,\n",
    ")\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-vectors\")\n",
    "emb1 = umapper1.fit_transform(iris.data, data_constrain=pin_mask)\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb1[11:15]\\n\",emb1[11:15])\n",
    "\n",
    "# Now demo pinning point 11 with a one-dimensional pin-mask\n",
    "pin_mask1d = np.ones( (emb0.shape[0]), dtype=np.int32)\n",
    "pin_mask1d[13] = 0.0\n",
    "pin_mask1d[13] = 0.0\n",
    "emb0[13] = [-4.0, 0]\n",
    "emb0[14] = [+4.0, 0]\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-values\")\n",
    "emb1 = umapper1.fit_transform(iris.data, data_constrain=pin_mask)\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb1[11:15]\\n\",emb1[11:15])\n",
    "print(\"\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### demo output_constrain and data_constrain\n",
    "A UMAP constraints function `y_bounder` restains *y*-values of any point to -5..+5\n",
    "This is independent of data set, so it is an `output_constrain` argument\n",
    "to the UMAP constructor.\n",
    "\n",
    "A `data_constrain` function is used to pin points 13 and 14 to specific positions.\n",
    "\n",
    "#### Tech note:\n",
    "- numba jit does **not** imbue @jitclass with sufficient support for callable objects.\n",
    "- So instead define `mk_FOO` functions with state in a local variable, that jit-compile\n",
    "  and return a dynamically jitted function.\n",
    "\n",
    "- This is not the nicest, but heh, it works.  see`mk_bound_y_values` for an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pinning embeddings of pts 13 and 14 to [-2,0] and [2,0]\")\n",
    "# pin embeddings of two data (13 and 14) to left and right of origin\n",
    "# via a custom constraint.  inf get no-op, other values get fixed\n",
    "import umap.constraints as con\n",
    "import numba\n",
    "infs0 = np.full_like(emb0, np.float32(np.inf), dtype=np.float32)\n",
    "infs0[13,:] = [-2.0,0]\n",
    "infs0[14,:] = [+2.0,0]\n",
    "@numba.njit()\n",
    "def constraint_idx_pt0(idx,pt):\n",
    "    con.freeinf_pt(idx,pt, infs0)\n",
    "# this function DOES depent on idx of pt\n",
    "    \n",
    "constraints = {\n",
    "    'idx_pt': constraint_idx_pt0,\n",
    "}\n",
    "# optional: set up the values to agree\n",
    "#emb0[13] = [-2.0, 0]\n",
    "#emb0[14] = [+2.0, 0]\n",
    "# Here is the \"move all points\" version of con.freeinf\n",
    "con.freeinf_pts(emb0, infs0)\n",
    "\n",
    "# Also demo a non-indexed (UMAP constructor) constraint,\n",
    "# that is independent of the iris.data.\n",
    "# Here, let's constrain 'y' to be within -5.0, +5.0\n",
    "# without, we got:\n",
    "# emb2[11:15]\n",
    "#  [[ 4.804111   3.430179 ]\n",
    "#  [ 4.2598176  7.352637 ]     # <-- 'y' is big here\n",
    "#  [-2.         0.       ]\n",
    "#  [ 2.         0.       ]]\n",
    "# With 'output_constrain':\n",
    "# emb2[11:15]\n",
    "# [[ 7.0536     2.275853 ]\n",
    "# [ 4.8699265  1.9713866]      # y constrained\n",
    "# [-2.         0.       ]\n",
    "# [ 2.         0.       ]]\n",
    "# So we pass illegal range for x, and legal range for y\n",
    "def mk_bound_y_values(lo, hi):\n",
    "    bound_los = np.array([+999.,lo], dtype=np.float32)\n",
    "    bound_his = np.array([-999.,hi], dtype=np.float32)\n",
    "    @numba.njit()\n",
    "    def bound_y_values(pt):\n",
    "        return con.dimlohi_pt(pt, bound_los, bound_his)\n",
    "    # this function does NOT depend on 'idx' arg\n",
    "    return bound_y_values\n",
    "y_bounder = mk_bound_y_values(-5.0,+5.0)\n",
    "\n",
    "# CHECK: x is unaffected, y range is bounded ...\n",
    "pt = np.array([1.,2.], dtype=np.float32)\n",
    "print(\"pt0\",pt); y_bounder(pt); print(\"pt0\",pt); assert pt[0] == 1.; assert pt[1] == 2.\n",
    "pt[0] = 10.; pt[1] = 10.\n",
    "print(\"pt1\",pt); y_bounder(pt); print(\"pt1\",pt); assert pt[0] == 10.; assert pt[1] == 5.\n",
    "pt[0] = -10.; pt[1] = -10.\n",
    "print(\"pt2\",pt); y_bounder(pt); print(\"pt2\",pt); assert pt[0] == -10.; assert pt[1] == -5.\n",
    "\n",
    "assert np.all(emb0[13] == [-2.0,0])\n",
    "assert np.all(emb0[14] == [+2.0,0])\n",
    "print(\"Specify 'init' embedding for umapper2\")\n",
    "# output_constraint allowed keys: pt grad epoch_pt final_pt\n",
    "umapper2 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12346, min_dist=0.001,\n",
    "    output_constrain = { 'pt': y_bounder }, # any pt, ind't of dataset\n",
    "    init=emb0, n_epochs=4,\n",
    ")\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-vectors\")\n",
    "# ... whereas data_constrain depends on the dataset (point number is important)\n",
    "# data_constraing allowed keys: idx_pt, idx_ipts, idx_grad\n",
    "emb2 = umapper2.fit_transform(iris.data, data_constrain=constraints)\n",
    "assert np.all(emb2[:,1] >= -5.0) and np.all(emb2[:,1] <= 5.0) # output_constrain\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb2[11:15]\\n\",emb2[11:15])\n",
    "print(\"\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user-defined constraint\n",
    "You can invent your own constraints.  Here we initialize and keep points\n",
    "13 and 14 on the x=y line.  We chose to do it here with a gradient-style constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"grad constraint 13 and 14 on line y=x\")\n",
    "# this one has little help from umap.constraints.py,\n",
    "# so define the numba constraint functions here:\n",
    "@numba.njit()\n",
    "def y_eq_x_pt(idx, pt):\n",
    "    avg = np.sum(pt) / pt.shape[0]\n",
    "    pt.fill(avg)\n",
    "@numba.njit()\n",
    "def y_eq_x_grad(idx, pt, grad):\n",
    "    # if we cannot assume pt satisfies constraints:\n",
    "    #y_eq_x_pt(idx, pt)  # put pt onto 45-degree line\n",
    "    # now tangent plane projection\n",
    "    y_eq_x_pt(idx, grad) # gradient also lies on the 45-degree line\n",
    "\n",
    "# pin embeddings of two data (13 and 14) to all-coords-equal line\n",
    "constraints = {\n",
    "    'idx_grad': y_eq_x_grad,\n",
    "}\n",
    "# init 13 and 14 to 45-degree line\n",
    "emb0[13,:] = [-1.0, -1.0]\n",
    "emb0[14,:] = [+1.0, +1.0]\n",
    "umapper3 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12346, min_dist=0.001,\n",
    "    init=emb0, n_epochs=2,\n",
    ")\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-vectors\")\n",
    "emb3 = umapper3.fit_transform(iris.data, data_constrain=constraints)\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb3[11:15]\\n\",emb3[11:15])\n",
    "np.testing.assert_allclose(emb3[13,0], emb3[13,1])\n",
    "np.testing.assert_allclose(emb3[14,0], emb3[14,1])\n",
    "print(\"\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### output_constrain to box, and data_constrain with some springs and some pins\n",
    "(Actually, pinning could also be done with a spring constant of $\\infty$.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.constraints2 as con\n",
    "import numba\n",
    "print(\"spring force constraint 13 and 14 pulled towards (0,3), (0,-3)\")\n",
    "# This shows a \"soft\" constraint, with no point-projection step,\n",
    "# and instead of projecting onto tangent space,\n",
    "# the gradients get modified by a simple user force.\n",
    "\n",
    "pin_idx = np.array([13,14], dtype=np.int32)\n",
    "springs = np.array([0.1, 0.01], dtype=np.float32)   # note np==inf **would** have projection constraint\n",
    "pin_pos = np.array([[0,3], [0,-3]], dtype=np.float32)\n",
    "print(\"pin_idx (anchors)         \", numba.typeof(pin_idx), \"\\n\", pin_idx)\n",
    "print(\"springs (force constants) \", numba.typeof(springs),  \"\\n\", springs)\n",
    "print(\"pin_pos (anchor positions)\", numba.typeof(pin_pos), \"\\n\", pin_pos)\n",
    "# pin point 12, but not via an infinite force spring... for show\n",
    "emb0[12,:] = [0.5, 0.5]\n",
    "my_pinned = np.array([12], dtype=np.int32)\n",
    "#@numba.njit\n",
    "#def pin12_grad(idx,pt):\n",
    "#    con.pinindexed_grad(idx,pt,grad,  my_pinned)\n",
    "@numba.njit\n",
    "def my_springs_and_pins(idx, pt, grad):\n",
    "    # pt is unconstrained\n",
    "    con.springindexed_grad(idx,pt, grad, pin_idx, pin_pos, springs)\n",
    "    # this is equivalent to an infinite force spring, but just for show...\n",
    "    con.pinindexed_grad(idx,pt, grad, my_pinned)\n",
    "# Note: we can only supply one function per dictionary key for constraints\n",
    "# This is unfortunate.  A list/tuple might be OK in upstream numba versions\n",
    "\n",
    "# second constraint (every pt inside simple box)\n",
    "my_los = np.full(2, -5.0, dtype=np.float32) # x and y low bound <- -5.0\n",
    "my_his = np.full(2, +5.0, dtype=np.float32)\n",
    "#original:\n",
    "#@numba.njit\n",
    "#def my_box(idx, pt):  # 'idx_pt' argument list\n",
    "#    con.dimlohi_pt(pt, my_los, my_his)\n",
    "# Note: idx is not needed -- this can now be supplied as an\n",
    "# 'output_constrain' value, to the UMAP constructor.\n",
    "@numba.njit\n",
    "def my_box2(pt):\n",
    "    con.dimlohi_pt(pt, my_los, my_his)\n",
    "# pin embeddings of two data (13 and 14) to all-coords-equal line\n",
    "constraints = {\n",
    "    #'idx_pt':   my_box, # better: this does not depend on 'idx',\n",
    "    #  so it's better to give 'output_constrain=my_box2' in UMAP constructor\n",
    "    'idx_grad': my_springs_and_pins,\n",
    "}\n",
    "# init 13 and 14 \"anywhere\"\n",
    "emb0[13,:] = [0, +5]\n",
    "emb0[14,:] = [0, -5]\n",
    "print(\"emb0[11:15] before my_box\\n\",emb0[11:15])\n",
    "umapper4 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12347, min_dist=0.001,\n",
    "    output_constrain = { 'pt': my_box2 },\n",
    "    init=emb0, n_epochs=4,\n",
    ")\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-vectors\")\n",
    "emb4 = umapper4.fit_transform(iris.data, data_constrain=constraints)\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb4[11:15]\\n\",emb4[11:15])\n",
    "print(\"pin_pos[0]\", pin_pos[0], \"distance:\",np.linalg.norm(emb4[13] - pin_pos[0]))\n",
    "print(\"pin_pos[1]\", pin_pos[1], \"distance:\",np.linalg.norm(emb4[14] - pin_pos[1]))\n",
    "print(\"\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we'll try out new constraint FUNCTION_ipts(idx, pts, ...)\n",
    "Uggh. new **_ipts** suffix to help numba disambiguate FUNCTION_pts(idx,pt,...)\n",
    "\n",
    "(I could not get numba to run-time dispatch this correctly)\n",
    "\n",
    "Following cell copies an early simple test, but with 'idx_ipts' constraint\n",
    "when calling umap `fit_transform`(...,data_constrain=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pinning embeddings of pts 13 and 14 to [-2.2,0] and [2.2,0]\")\n",
    "# pin embeddings of two data (13 and 14) to left and right of origin\n",
    "# via a custom constraint.  inf get no-op, other values get fixed\n",
    "import umap.constraints as con\n",
    "import numba\n",
    "infs0 = np.full_like(emb0, np.float32(np.inf), dtype=np.float32)\n",
    "infs0[13,:] = [-2.2,0]\n",
    "infs0[14,:] = [+2.2,0]\n",
    "@numba.njit(\"f4[:](i8, f4[:,:])\")\n",
    "def constraint_idx_pt1(idx,pts):\n",
    "    # Here 'pts' MUST be 2D (full point cloud)\n",
    "    #   (probably equiv. to constrain_idx_pt0(idx, pts[idx,:])\n",
    "    return con.freeinf_ipts(idx,pts, infs0)\n",
    "# this function DOES depent on idx of pt\n",
    "    \n",
    "constraints = {\n",
    "    'idx_ipts': constraint_idx_pt1,  # NEW _ipts suffix ==> alt call signature\n",
    "    # actually this is most flexible since the full \"tail_embedding\"\n",
    "    # point cloud is actually always available.\n",
    "}\n",
    "emb0[13] = [-3.14, 3.14] # init with far away coords\n",
    "emb0[14] = [+3.14, 3.14]\n",
    "# optional: set up the values to agree\n",
    "#emb0[13] = [-2.0, 0]\n",
    "#emb0[14] = [+2.0, 0]\n",
    "#assert np.all(emb0[13] == [-2.0,0])\n",
    "#assert np.all(emb0[14] == [+2.0,0])\n",
    "# Here is the \"move all points\" version of con.freeinf\n",
    "con.freeinf_pts(emb0, infs0)\n",
    "\n",
    "# Also demo a non-indexed (UMAP constructor) constraint,\n",
    "# that is independent of the iris.data.\n",
    "# Here, let's constrain 'y' to be within -5.0, +5.0\n",
    "# without, we got:\n",
    "# emb2[11:15]\n",
    "#  [[ 4.804111   3.430179 ]\n",
    "#  [ 4.2598176  7.352637 ]     # <-- 'y' is big here\n",
    "#  [-2.         0.       ]\n",
    "#  [ 2.         0.       ]]\n",
    "# With 'output_constrain':\n",
    "# emb2[11:15]\n",
    "# [[ 7.0536     2.275853 ]\n",
    "# [ 4.8699265  1.9713866]      # y constrained\n",
    "# [-2.         0.       ]\n",
    "# [ 2.         0.       ]]\n",
    "# So we pass illegal range for x, and legal range for y\n",
    "def mk_bound_y_values(lo, hi):\n",
    "    bound_los = np.array([+999.,lo], dtype=np.float32)\n",
    "    bound_his = np.array([-999.,hi], dtype=np.float32)\n",
    "    @numba.njit()\n",
    "    def bound_y_values(pt):\n",
    "        return con.dimlohi_pt(pt, bound_los, bound_his)\n",
    "    # this function does NOT depend on 'idx' arg\n",
    "    return bound_y_values\n",
    "y_bounder = mk_bound_y_values(-5.0,+5.0)\n",
    "\n",
    "# CHECK: x is unaffected, y range is bounded ...\n",
    "pt = np.array([1.,2.], dtype=np.float32)\n",
    "print(\"pt0\",pt); y_bounder(pt); print(\"pt0\",pt); assert pt[0] == 1.; assert pt[1] == 2.\n",
    "pt[0] = 10.; pt[1] = 10.\n",
    "print(\"pt1\",pt); y_bounder(pt); print(\"pt1\",pt); assert pt[0] == 10.; assert pt[1] == 5.\n",
    "pt[0] = -10.; pt[1] = -10.\n",
    "print(\"pt2\",pt); y_bounder(pt); print(\"pt2\",pt); assert pt[0] == -10.; assert pt[1] == -5.\n",
    "\n",
    "print(\"Specify 'init' embedding for umapper2\")\n",
    "umapper5 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12346, min_dist=0.001,\n",
    "    output_constrain = { 'pt': y_bounder }, # any pt, ind't of dataset\n",
    "    init=emb0, n_epochs=4,\n",
    ")\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-vectors\")\n",
    "# ... whereas data_constrain depends on the dataset (point number is important)\n",
    "emb5 = umapper5.fit_transform(iris.data, data_constrain=constraints)\n",
    "assert np.all(emb5[:,1] >= -5.0) and np.all(emb5[:,1] <= 5.0) # output_constrain\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb5[11:15]\\n\",emb5[11:15])\n",
    "assert np.allclose(emb5[13], [-2.2,0])\n",
    "assert np.allclose(emb5[14], [+2.2,0])\n",
    "print(\"\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We were asked for a clusterer.\n",
    "Every time the cluster membership changes, you create a clustering function.\n",
    "\n",
    "-API **mk_clusters(clusters, springs, *, mindist=None, mults=None, wm=None)**\n",
    "  - input as list-of-lists `clusters[c][i]`: (i) cluster(`c`=0,1,...), (ii) points (indices `idx`) in cluster.\n",
    "    - also internally remember map of index to clusters: `clusts[idx] = list-of-cluster(`c`)`\n",
    "    - also internally calculate cluster size `cl_n[c]`\n",
    "    - and cluster average position `cl_avg[c]`\n",
    "    - opt. store both as numpy sparse matrix + sparse transpose ?\n",
    "  - input spring force `springs[c]` and opt. learning rate lr=1.0\n",
    "  - TODO: input optional de-neighboring `mults[c]` (default 1.0) and modify umap weight matrix `wm`\n",
    "  - input optional mindist for short-range cluster repulsive force (Morse function variant?)\n",
    "  - output jit func (`idx`, `pts`)\n",
    "    - for `c` in `clusts[idx]`:  # all clusters c of idx\n",
    "      - `delta` of pt moved toward `cl_avg[c]` with `springs[c]`\n",
    "    - TODO (maybe)\n",
    "      - quick-update `cl_avg[c]` by `(delta / cl_n[c])`\n",
    "      - opt. every 100'th call, recalculate exact cluster averages instead of `delta`-update\n",
    "      - opt. short-range repulsion to other neighbors of `idx` in cluster `c`\n",
    "\n",
    "- **do** support a pt assigned to multiple clusters\n",
    "  - calc eqm_pos and total spring force from all clusters\n",
    "  - move there *without* applying `mindist`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let's demo python clustering internals (no-debug version)\")\n",
    "#\n",
    "# --------------------- Inputs -----------------\n",
    "#\n",
    "n_samples = 8\n",
    "cluster_lists = [[1,2,3], [2,7,6]]\n",
    "#           avg    2.0      5.0\n",
    "# each of the 2 clusters has a spring constant\n",
    "springs = np.array([0.8, 2.0])\n",
    "lr = 1.0   # learning rate (gradient multiplier) for spring forces\n",
    "\n",
    "pts = np.ndarray((n_samples,2), dtype=np.float32)\n",
    "for i in range(n_samples):\n",
    "    pts[i,:] = (float(i),float(i%3))\n",
    "\n",
    "n_samples = 8\n",
    "cluster_lists = [[1,2,3], [2,7,6]]\n",
    "#           avg    2.0      5.0\n",
    "# each of the 2 clusters has a spring constant\n",
    "springs = np.array([0.8, 2.0])\n",
    "lr = 1.0   # learning rate (gradient multiplier) for spring forces\n",
    "\n",
    "pts = np.ndarray((n_samples,2), dtype=np.float32)\n",
    "for i in range(n_samples):\n",
    "    pts[i,:] = (float(i),float(i%3))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#\n",
    "# ---------------------- Functions --------------\n",
    "#\n",
    "\n",
    "# begin with 2 very simple helpers\n",
    "@numba.njit()\n",
    "def np_mean_axis_0(pts):\n",
    "    cl_avg = np.zeros((pts.shape[1]), dtype=np.float32)\n",
    "    for pt in range(pts.shape[0]):\n",
    "        cl_avg += pts[pt,:]\n",
    "    assert pts.shape[0] > 0\n",
    "    cl_avg /= pts.shape[0]\n",
    "    return cl_avg\n",
    "\n",
    "@numba.njit() # eventually should just inline this wherever\n",
    "def xnp_cluster_list(c, clusters):\n",
    "    return np.argwhere(clusters[c,:]).flatten()\n",
    "\n",
    "@numba.njit()\n",
    "def spring_mv_v0(pt, target, spring, lr=1.0, mindist=0.0):\n",
    "    vec = target - pt      # vector toward cluster center\n",
    "    if mindist > 0.0:\n",
    "        vecnorm = np.linalg.norm(vec)\n",
    "        bar = max(vecnorm - mindist, 0.0) # max move dist (along vec/vecnorm)\n",
    "        #delta = (min(lr * spring, 1.0) * bar / vecnorm) * vec\n",
    "        delta = (min(bar, bar * lr * spring) / vecnorm) * vec\n",
    "    else: # mindist == 0.0\n",
    "        delta = min(1.0, lr*spring) * vec\n",
    "    return delta\n",
    "\n",
    "#@numba.njit()\n",
    "def do_clustering_ipts(idx, pts, cluster_lists, springs, *, lr=1.0, mindist=0.01):\n",
    "    assert len(pts.shape) == 2\n",
    "    n_samples = pts.shape[0]\n",
    "    n_clust = len(cluster_lists)\n",
    "    assert n_clust > 0\n",
    "    assert len(springs) == len(cluster_lists)\n",
    "    np_cluster_lists = [np.array(clist,np.int32) for clist in cluster_lists]\n",
    "    np_springs = np.array(springs)\n",
    "    #\n",
    "    # cluster membership as bool array[cluster index][point index]\n",
    "    #\n",
    "    clusters = np.full((n_clust, n_samples), False, dtype=bool)\n",
    "    for (c,members) in enumerate(cluster_lists):\n",
    "        for m in members:\n",
    "            clusters[c][m] = True\n",
    "    \n",
    "    idx_in = clusters[:,idx]\n",
    "    idx_cl = np.argwhere(idx_in).flatten()\n",
    "    #\n",
    "    if len(idx_cl) < 1:\n",
    "        return\n",
    "\n",
    "    elif len(idx_cl) == 1:\n",
    "        # Separate out an easy case (idx in single cluster)\n",
    "        c = idx_cl[0]\n",
    "        cluster_pts  = np_cluster_lists[c] # perhaps faster/vectorizable\n",
    "        if len(cluster_pts) <= 1:\n",
    "            print(\"noop: cluster empty or of size 1\")\n",
    "            return\n",
    "\n",
    "        # cl_avg ~ cluster center\n",
    "        cl_avg = np.mean(pts[cluster_pts,:], axis=0)\n",
    "        cl_avg2 = np_mean_axis_0( pts[cluster_pts,:] )  # cluster centroid\n",
    "        #print(f\"{idx=} {cl_avg=}\\n{cl_avg2=}\")\n",
    "\n",
    "        delta = spring_mv_v0(pts[idx], cl_avg, springs[c], lr, mindist)\n",
    "        pts[idx,:] += delta\n",
    "\n",
    "        # A quick approx update of cl_avg feasible, but don't use it for now\n",
    "        #cl_avg_new = cl_avg + delta/len(cluster_pts)\n",
    "        # It's trickier for len(idx_cl) > 1 (more updates needed!)\n",
    "\n",
    "        return\n",
    "\n",
    "    assert( len(idx_cl) > 1 )\n",
    "    # Easy: if point is in two clusters, pt moves first toward one,\n",
    "    #       then toward next, in SAME pattern.\n",
    "    #\n",
    "    # Actual: 1st determine NET gradient direction from\n",
    "    #         weighted sum of individual gradient forces,\n",
    "    #         with movement not to overshoot weighted \"equilibrium\"\n",
    "    #         position derived from weighted avg of cluster centers\n",
    "    #\n",
    "    tot_grad = np.zeros(pts.shape[1],dtype=np.float32)\n",
    "    eqm_pos  = np.zeros(pts.shape[1],dtype=np.float32)\n",
    "    sum_springs = 0.0 #np.sum(np_springs)\n",
    "    n_springs = 0\n",
    "    if False: # first way, 2 loops\n",
    "        cl_avgs = []\n",
    "        for c in idx_cl:\n",
    "            cluster_pts  = np_cluster_lists[c] # perhaps faster/vectorizable\n",
    "            if len(cluster_pts) < 1:\n",
    "                cl_avgs.append(None)\n",
    "                print(\"noop: cluster empty or of size 1\")\n",
    "                continue\n",
    "            # cl_avg ~ cluster center\n",
    "            cl_avg = np.mean(pts[cluster_pts,:], axis=0)\n",
    "            cl_avgs.append(cl_avg)\n",
    "            n_springs += 1\n",
    "\n",
    "        tot_grad = np.zeros(pts.shape[1],dtype=np.float32)\n",
    "        eqm_pos  = np.zeros(pts.shape[1],dtype=np.float32)\n",
    "        sum_springs = np.sum(np_springs) # correct if all clusters non-empty\n",
    "\n",
    "        # calculate equilibrium spring-weight target position\n",
    "        #           and total spring-weighted gradient\n",
    "        # every cluster exerts a force ind't of cluster size\n",
    "        for (i,c) in enumerate(idx_cl):\n",
    "            if cl_avgs[i] is not None: # i.e. the cluster is non-empty, has a centroid\n",
    "                # springs -> force gradient -> non-overshooting 'delta' movement\n",
    "                eqm_pos += np_springs[c] * cl_avgs[i]\n",
    "                vec = cl_avgs[i] - pts[idx,:]      # vector toward cluster center\n",
    "                tot_grad += np_springs[c] * vec\n",
    "\n",
    "        eqm_pos /= sum_springs\n",
    "    \n",
    "    else: # shorter way: combine loops\n",
    "\n",
    "        # calculate equilibrium spring-weight target position\n",
    "        #           and total spring-weighted gradient\n",
    "        # every cluster exerts a force ind't of cluster size\n",
    "        for c in idx_cl:\n",
    "            cluster_pts  = xnp_cluster_list(c, clusters) # perhaps faster/vectorizable\n",
    "            # Note: 1. Each cluster centroid gets its spring regardless\n",
    "            #          of how populated the cluster is.\n",
    "            #       2. Size 1 cluster get included\n",
    "            #       3. Spring constant doubles as weighting factor -- this\n",
    "            #          might not hold for other spring force models!\n",
    "            if len(cluster_pts) > 0:\n",
    "                cl_avg = np_mean_axis_0( pts[cluster_pts,:] )\n",
    "                #print(f\"{c=} {cl_avg=}\")\n",
    "                # Now update sums for equilibrium posn and total gradient\n",
    "                # springs -> summed force gradient vectors\n",
    "                spring = np_springs[c]\n",
    "                sum_springs += spring\n",
    "                n_springs += 1\n",
    "                eqm_pos += spring * cl_avg\n",
    "                vec = cl_avg - pts[idx,:]      # vector toward cluster center\n",
    "                tot_grad += spring * vec       # only for kr^2 spring physics\n",
    "\n",
    "        eqm_pos /= sum_springs\n",
    "\n",
    "    print(f\"{eqm_pos=}\")\n",
    "\n",
    "    if True: # assert\n",
    "        # tot_grad should automatically be in direction of eqm_pos\n",
    "        # if math is correct:\n",
    "        grad_dirn = tot_grad / np.linalg.norm(tot_grad)\n",
    "        eqm_dirn  = eqm_pos - pts[idx,:]\n",
    "        eqm_dirn /= np.linalg.norm(eqm_dirn)\n",
    "        assert( np.allclose(grad_dirn, eqm_dirn) )\n",
    "\n",
    "    # I don't think mindist should apply to \"virtual\" avg-of-cluster-centroids\n",
    "    use_mindist_for_eqm = False\n",
    "    \n",
    "    if False and not use_mindist_for_eqm: # orig (no mindist)\n",
    "        #vec = eqm_pos - pts[idx]      # vector toward spring-weighted equilibrium\n",
    "        #vecnorm = np.linalg.norm(vec)\n",
    "        eqm_spring = np.linalg.norm(tot_grad) / np.linalg.norm(eqm_pos-pts[idx])\n",
    "        vec = eqm_pos - pts[idx]      # vector toward spring-weighted equilibrium\n",
    "        vecnorm = np.linalg.norm(vec)\n",
    "        print(f\"{eqm_spring=} {eqm_spring*vecnorm=}\")\n",
    "\n",
    "        # at this point we have net gradient and terminal eqm_pos\n",
    "        # we proceed as before, so that the force is toward eqm_pos\n",
    "        # but the point never overshoots.\n",
    "        #\n",
    "        # Can we use an \"effective spring constant\" for eqm_pos?\n",
    "        # or do we need a 'gradnorm'-based updater?\n",
    "        #\n",
    "        vec = eqm_pos - pts[idx]      # vector toward spring-weighted equilibrium\n",
    "        vecnorm = np.linalg.norm(vec)\n",
    "        gradnorm = np.linalg.norm(tot_grad)\n",
    "        # spring-physics model:\n",
    "        #    gradnorm = spring_effx * (displacement=vecnorm)\n",
    "        spring_effx = gradnorm / vecnorm\n",
    "        print(f\"{vecnorm=} {gradnorm=} {spring_effx=}\")\n",
    "        #   v.0: by simply capping movement to \"100% of the way to cluster avg\"\n",
    "        #        for movement to eqm posn, can move all the way?\n",
    "        pct_move = min(1.0, (lr * gradnorm)/vecnorm)\n",
    "        #   v.1: even for a move to eqm_pos, use a mindist2 guaranteed \"small\"\n",
    "        #mindist2 = min( mindist, 0.1 )\n",
    "        #pct_move = smoothstep( lr * gradnorm, vecnorm - 0.5*mindist2, vecnorm - mindist2 )\n",
    "        # above is WRONG\n",
    "        delta = vec * pct_move\n",
    "        \n",
    "        #def spring_mv_v0_plain(pt, target, spring, lr=1.0, mindist=0.0):\n",
    "        #   vec = target - pt      # vector toward cluster center\n",
    "        #    if mindist > 0.0:\n",
    "        #        vecnorm = np.linalg.norm(vec)\n",
    "        #        bar = max(vecnorm - mindist, 0.0) # max move dist (along vec/vecnorm)\n",
    "        #        delta = (min(lr * spring, 1.0) * bar / vecnorm) * vec\n",
    "        #    else: # mindist == 0.0\n",
    "        #        delta = min(1.0, lr*spring) * vec\n",
    "        #    return delta\n",
    "        # with above cancellation, for mindist==0.0, we have just:\n",
    "        \n",
    "    elif True and not use_mindist_for_eqm: # orig (no mindist) -- extremely simple!\n",
    "        # almost too simple to be true\n",
    "        vec = eqm_pos - pts[idx]      # vector toward spring-weighted equilibrium\n",
    "        gradnorm = np.linalg.norm(tot_grad)\n",
    "        delta = min(1.0, lr * gradnorm) * vec\n",
    "        #                     ^^^^^^^^ replaces 'spring'\n",
    "        \n",
    "    else: # with min_dist : now the \"target\" is not eqm_pos, but a point mindist away from it\n",
    "        # In principle:\n",
    "        #eqm_spring = np.linalg.norm(tot_grad) / max(1.e-6, (np.linalg.norm(eqm_pos-pts[idx]) - mindist))\n",
    "        # But for targets that are \"midway\" between several clusters, maybe it is best to NOT\n",
    "        # use mindist\n",
    "        #delta = spring_mv_v0(pts[idx], eqm_pos, eqm_spring, lr, mindist)\n",
    "        #\n",
    "        # or long-hand:\n",
    "        vec = eqm_pos - pts[idx]            # vector toward spring-weighted equilibrium\n",
    "        vecnorm = np.linalg.norm(vec)\n",
    "        bar = max(vecnorm - mindist, 0.0)   # max move dist (along vec/vecnorm)\n",
    "        gradnorm = np.linalg.norm(tot_grad) # this replaces 'spring' in spring_mv_v0_plain\n",
    "        delta = (min(lr * gradnorm, 1.0) * bar / vecnorm) * vec\n",
    "\n",
    "\n",
    "    if True:\n",
    "        pts[idx,:] = pts[idx,:] + delta\n",
    "    else: # verbose\n",
    "        newpos = pts[idx,:] + delta\n",
    "        print(f\"pt {idx} {int(pct_move*100.)}% {pts[idx,:]} --> newpos {newpos}\")\n",
    "        pts[idx,:] = newpos\n",
    "        print(f\"final {pts[idx,:]=}\")\n",
    "\n",
    "\n",
    "    return\n",
    "\n",
    "#@numba.njit()\n",
    "def do_clustering_pts(pts, cluster_lists, springs, *, lr=1.0, mindist=0.01):\n",
    "    assert len(pts.shape) == 2\n",
    "    n_samples = pts.shape[0]\n",
    "    # one round of clustering every point once\n",
    "    for idx in range(n_samples):\n",
    "        #pt0 = pts[idx,:].copy()\n",
    "        do_clustering_ipts(idx, pts, cluster_lists, springs, lr=lr, mindist=mindist)\n",
    "        #print(f\"{idx=} {pt0=} --> {pts[idx,:]=}\")\n",
    "    return\n",
    "\n",
    "print(f\"{type(pts)=} {pts.shape=} {pts.dtype=}\")\n",
    "do_clustering_pts(pts, cluster_lists, springs, lr=lr, mindist=0.2)\n",
    "print(\"final positions (one epoch)\")\n",
    "for i in range(pts.shape[0]):\n",
    "    print(f\"{i=} pt = {pts[i,:]}\")\n",
    "\n",
    "if False: #pts_ref is not None:\n",
    "    assert( np.allclose(pts, pts_ref) )\n",
    "    print(\"Good: matched pts_ref\")\n",
    "else:\n",
    "    pts_ref = pts.copy()\n",
    "print(\"Goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "print(\"Let's demo python clustering internals (jit version)\")\n",
    "print(\"This cell is the precursor a new file umap/constrain_clust.py\\n\")\n",
    "\n",
    "#\n",
    "# ---------------------- Functions --------------\n",
    "# ouch.  these must be jittable now\n",
    "# numba cannot handle a python list of numpy arrays.\n",
    "# Let's break things apart to find numba-ready code blocks\n",
    "#\n",
    "@numba.njit()\n",
    "def np_mean_axis_0(pts):\n",
    "    cl_avg = np.zeros((pts.shape[1]), dtype=np.float32)\n",
    "    for pt in range(pts.shape[0]):\n",
    "        cl_avg += pts[pt,:]\n",
    "    assert pts.shape[0] > 0\n",
    "    cl_avg /= pts.shape[0]\n",
    "    return cl_avg\n",
    "\n",
    "@numba.njit() # approx. void(i8, f4[:,:], i8[:], f8, f8)\n",
    "#def xdo_clustering_single(idx, pts, cluster_pts, lr, spring):\n",
    "def xdo_clustering_ipts_toward0(idx, pts, target, lr, spring, maxfrac=0.9):\n",
    "    \"\"\" Non-overshooting move of pts[idx,:] towards cofm(pts[cluster_pts]).\n",
    "    \n",
    "        In this version, maxfrac=0.9 is an under-relaxation \"don't go all the way\".\n",
    "    \"\"\"\n",
    "    if lr*spring > 1e-5:\n",
    "        vec = target - pts[idx,:]\n",
    "        vecsz = np.linalg.norm(vec)         # stepsz is distance ~ spring force * lr\n",
    "        stepsz = lr * (spring * vecsz)\n",
    "        fracsz = min(maxfrac, stepsz/vecsz) # maxfrac<1 => undershoot\n",
    "        pts[idx,:] += fracsz * vec\n",
    "    # v.1  attempt to smooth the transition\n",
    "    #vec = cl_avg - pts[idx]      # vector toward cluster center\n",
    "    #vecnorm = np.linalg.norm(vec)\n",
    "    #if vecnorm <= mindist: # no-op - pt is already within mindist of cl_avg\n",
    "    #    delta = vec * 0.0\n",
    "    #else:\n",
    "    #    grad = np_springs[c] * vecnorm\n",
    "    #    gradnorm = lr * grad\n",
    "    #    edge0 = max(vecnorm - 2*mindist, 0.0)\n",
    "    #    edge1 = vecnorm - mindist\n",
    "    #    if gradnorm <= edge0:\n",
    "    #        delta = vec\n",
    "    #    else:\n",
    "    #        mvlen = edge0 + (edge1-edge0) * smoothstep( gradnorm, edge0, edge1 )\n",
    "    #        delta = vec * (mvlen/vecnorm)\n",
    "    #delta = vec * pct_move\n",
    "    return\n",
    "    #\n",
    "    # compare with multi-cluster case, where we calc tot_grad up front\n",
    "    # as a spring-weighted sum of force vectors\n",
    "    #\n",
    "    #if True:\n",
    "    #    # at this point we have net gradient and terminal eqm_pos\n",
    "    #    # we proceed as before, so that the force is toward eqm_pos\n",
    "    #    # but the point never overshoots.\n",
    "    #    #   v.0: by simply capping movement to \"100% of the way to cluster avg\"\n",
    "    #    #        for movement to eqm posn, can move all the way?\n",
    "    #    eqm_pos = target\n",
    "    #    vec = eqm_pos - pts[idx]\n",
    "    #    vecnorm = np.linalg.norm(vec)\n",
    "    #    gradnorm = np.linalg.norm(tot_grad) # gradnorm is NOT spring * vecnorm\n",
    "    #    pct_move = min(1.0, (lr * gradnorm) / vecnorm)\n",
    "    #    delta = vec * pct_move\n",
    "    #\n",
    "    # this can be reproduced by an \"effective spring const\"\n",
    "    #   gradnorm = np.linalg.norm(tot_grad)\n",
    "    # == ?\n",
    "    #   spring_eff * np.linalg.norm(target-pts[idx])\n",
    "    # IFF\n",
    "    #   spring_eff = np.linalg.norm(target-pts[idx]) / np.linalg.norm(tot_grad)\n",
    "    #\n",
    "\n",
    "#if False: # for reference\n",
    "#    def spring_mv_v0_plain(pt, target, spring, lr=1.0, mindist=0.0):\n",
    "#        vec = target - pt      # vector toward cluster center\n",
    "#        vecnorm = np.linalg.norm(vec)\n",
    "#        if mindist > 0.0:\n",
    "#            bar = max(vecnorm - mindist, 0.0) # max move dist (along vec/vecnorm)\n",
    "#            delta = (min(lr * spring, 1.0) * bar / vecnorm) * vec\n",
    "#        else: # mindist == 0.0\n",
    "#            delta = min(1.0, lr*spring) * vec\n",
    "#        return delta\n",
    "\n",
    "@numba.njit() # approx. void(i8, f4[:,:], i8[:], f8, f8)\n",
    "def xdo_clustering_ipts_toward(idx, pts, target, lr, spring, mindist=0.0):\n",
    "    \"\"\" Non-overshooting move of pts[idx,:] towards cofm(pts[cluster_pts]).\n",
    "    \n",
    "        lr: time step (1.0 will move exactly to equilibrium posn if spring==1)\n",
    "        \n",
    "        spring : spring constant \"force ~ spring * displacement\" (parabolic potential)\n",
    "        \n",
    "        mindist: to \"not go all the way\" towards target, but stop mindist away.\n",
    "    \"\"\"\n",
    "    # Using maxfrac:\n",
    "    #if lr*spring > 1e-5:\n",
    "    #    vec = target - pts[idx,:]\n",
    "    #    vecsz = np.linalg.norm(vec)         # stepsz is distance ~ spring force * lr\n",
    "    #    stepsz = lr * (spring * vecsz)\n",
    "    #    fracsz = min(maxfrac, stepsz/vecsz) # maxfrac<1 => undershoot\n",
    "    #    pts[idx,:] += fracsz * vec\n",
    "    #return\n",
    "    \n",
    "    # mindist python move fn:\n",
    "    #def spring_mv_v0_plain(pt, target, spring, lr=1.0, mindist=0.0):\n",
    "    #   vec = target - pt      # vector toward cluster center\n",
    "    #    if mindist > 0.0:\n",
    "    #        vecnorm = np.linalg.norm(vec)\n",
    "    #        bar = max(vecnorm - mindist, 0.0) # max move dist (along vec/vecnorm)\n",
    "    #        delta = (min(lr * spring, 1.0) * bar / vecnorm) * vec\n",
    "    #  or just delta = (min(bar, bar * lr * spring) / vecnorm) * vec\n",
    "    #    else: # mindist == 0.0\n",
    "    #        delta = min(1.0, lr*spring) * vec\n",
    "    #    return delta\n",
    "    \n",
    "    # movement with mindist (patterned after python helper \"spring_mv_v0\")\n",
    "    vec = target - pts[idx,:]      # vector toward cluster center\n",
    "    vecnorm = np.linalg.norm(vec)\n",
    "    if vecnorm > 1e-5:\n",
    "        bar = max(vecnorm - mindist, 0.0) # max move dist (along vec/vecnorm)\n",
    "        #delta = (min(lr * spring, 1.0) * bar / vecnorm) * vec\n",
    "        mv = min(bar, bar * lr * spring)\n",
    "        pts[idx,:] += (mv/vecnorm) * vec\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "@numba.njit() # eventually should just inline this wherever\n",
    "def xnp_cluster_list(c, clusters):\n",
    "    return np.argwhere(clusters[c,:]).flatten()\n",
    "\n",
    "@numba.njit(\n",
    "    locals={'tot_grad': numba.float32[:],\n",
    "            'eqm_pos' : numba.float32[:],\n",
    "            'sum_springs' : numba.float32,\n",
    "            'n_springs'   : numba.int64,\n",
    "           }\n",
    ")\n",
    "def xdo_clustering_mult(idx, idx_cl, clusters, springs, pts):\n",
    "    \"\"\" return target+grad info for idx w/ springs to  multiple clusters. \"\"\"\n",
    "    tot_grad = np.zeros(pts.shape[1],dtype=np.float32)\n",
    "    eqm_pos  = np.zeros(pts.shape[1],dtype=np.float32)\n",
    "    sum_springs = 0.0 #np.sum(np_springs)\n",
    "    n_springs = 0\n",
    "\n",
    "    # calculate equilibrium spring-weight target position\n",
    "    #           and total spring-weighted gradient\n",
    "    # every cluster exerts a force ind't of cluster size\n",
    "    for cc in idx_cl:\n",
    "        cluster_pts  = xnp_cluster_list(cc, clusters) # perhaps faster/vectorizable\n",
    "        # Note: 1. Each cluster centroid gets its spring regardless\n",
    "        #          of how populated the cluster is.\n",
    "        #       2. Size 1 cluster get included\n",
    "        #       3. Spring constant doubles as weighting factor -- this\n",
    "        #          might not hold for other spring force models!\n",
    "        if len(cluster_pts) > 0:\n",
    "            cl_avg = np_mean_axis_0( pts[cluster_pts,:] )\n",
    "            #print(f\"{cc=} {cluster_pts=}{cl_avg=}\")\n",
    "            # Now update sums for equilibrium posn and total gradient\n",
    "            # springs -> force gradient -> non-overshooting 'delta' movement\n",
    "            spring = springs[cc]\n",
    "            sum_springs += spring\n",
    "            n_springs += 1\n",
    "            eqm_pos += spring * cl_avg\n",
    "            vec = cl_avg - pts[idx,:]      # vector toward cluster center\n",
    "            tot_grad += spring * vec       # only for kr^2 spring physics\n",
    "    #\n",
    "    eqm_pos /= sum_springs\n",
    "\n",
    "    return (n_springs, eqm_pos, tot_grad)\n",
    "\n",
    "@numba.njit(\"void(i8, f4[:,:], boolean[:,:], f4[:], f8, f8)\")\n",
    "def xdo_clustering_ipts(idx, pts, clusters, springs, lr=1.0, mindist=0.01):\n",
    "    #assert len(pts.shape) == 2\n",
    "    n_samples = pts.shape[0]\n",
    "    n_clust = springs.shape[0] # it is a vector, one per cluster\n",
    "    #  otherwise n_clust = np.max(clusters) + 1\n",
    "    #n_clust = len(np_cluster_lists)\n",
    "    #assert n_clust > 0\n",
    "    #assert len(springs) == len(np_cluster_lists)\n",
    "    #np_cluster_lists = [np.array(clist,np.int32) for clist in cluster_lists]\n",
    "    #np_springs = np.array(springs)\n",
    "    idx_in = clusters[:,idx]\n",
    "    idx_cl = np.argwhere(idx_in).flatten()\n",
    "\n",
    "    if len(idx_cl) < 1:\n",
    "        return\n",
    "\n",
    "    elif len(idx_cl) == 1:\n",
    "        # Separate out an easy case (idx in single cluster)\n",
    "        c = idx_cl[0]\n",
    "        cluster_pts  = xnp_cluster_list(c, clusters)\n",
    "        # pts[idx] in a cluster of size 1 is already at the cluster centroid\n",
    "        if len(cluster_pts) > 1:\n",
    "            centroid = np_mean_axis_0( pts[cluster_pts,:] )  # cluster centroid\n",
    "            #pt0 = pts[idx,:].copy()\n",
    "            xdo_clustering_ipts_toward(idx, pts, centroid,\n",
    "                                       lr, springs[c], mindist=mindist)\n",
    "            #print(f\"{idx=} {c=} {centroid=}\\n{pt0=} --> {pts[idx]}\")\n",
    "        return\n",
    "\n",
    "    #else: idx attracted to multiple clusters... len(idx_cl) > 1 ... rare?\n",
    "    # jit test:\n",
    "    (n_springs, eqm_pos, tot_grad) = xdo_clustering_mult(idx, idx_cl, clusters, springs, pts)\n",
    "    \n",
    "    if n_springs==0:  # all clusters empty? No-op\n",
    "        return\n",
    "\n",
    "    if True:\n",
    "        # multi-cluster effective spring constant, to re-use \"_toward\" code...\n",
    "        #  This way re-uses existing code but redoes some vector calcs\n",
    "        eqm_spring = np.linalg.norm(tot_grad) / np.linalg.norm(eqm_pos-pts[idx])\n",
    "        #xdo_clustering_ipts_toward0(idx, pts, eqm_pos, lr, eqm_spring, maxfrac=1.0)\n",
    "        xdo_clustering_ipts_toward(idx, pts, eqm_pos, lr, eqm_spring, mindist=0.0)\n",
    "    elif False:\n",
    "        # Actually, with mindist 0 the calc is very simple, so longhand:\n",
    "        \n",
    "        # The calculation with mindist:\n",
    "        #vec = target - pt      # vector toward cluster center\n",
    "        #vecnorm = np.linalg.norm(vec)\n",
    "        #if vecnorm > 1e-5:\n",
    "        #    bar = max(vecnorm - mindist, 0.0) # max move dist (along vec/vecnorm)\n",
    "        #    mv = min(bar, lr * bar * spring)\n",
    "        #    pts[idx,:] += (mv/vecnorm) * vec\n",
    "        #return\n",
    "        \n",
    "        # without minidst (i.e. bar = vecnorm)\n",
    "        vec = eqm_pos - pts[idx]      # vector toward spring-weighted equilibrium\n",
    "        vecnorm = np.linalg.norm(vec)\n",
    "        if vecnorm > 1e-5:\n",
    "            mv = lr * np.linalg.norm(tot_grad)  # lr : force --> distance\n",
    "            mv = min(mv, vecnorm)\n",
    "            pts[idx,:] += (mv/vecnorm) * vec\n",
    "            #\n",
    "            #pts[idx,:] += ((min(vecnorm, lr*np.linalg.norm(tot_grad)) / vecnorm) * vec\n",
    "            #\n",
    "            #pts[idx,:] += (min(1.0, lr*np.linalg.norm(tot_grad))) * vec\n",
    "    else: # \"longhand\" (so simple for mindist=0.0)\n",
    "        vec = eqm_pos - pts[idx]      # vector toward spring-weighted equilibrium\n",
    "        mv = lr * np.linalg.norm(tot_grad)\n",
    "        #pts[idx,:] += min(1.0, mv) * vec\n",
    "        if mv < 1.0:\n",
    "            pts[idx,:] += mv * vec\n",
    "        else:\n",
    "            pts[idx,:] = eqm_pos\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "#@numba.njit()\n",
    "def xdo_clustering_pts(pts, cluster2d, springs, lr=1.0, mindist=0.01):\n",
    "    assert len(pts.shape) == 2\n",
    "    n_samples = pts.shape[0]\n",
    "    # one round of clustering every point once\n",
    "    for idx in range(n_samples):\n",
    "        xdo_clustering_ipts(idx, pts, cluster2d, springs, lr, mindist)\n",
    "    return\n",
    "\n",
    "#\n",
    "# -------------- python (non-numba) --------------\n",
    "# These show how to convert python args to expected numba-compliant types\n",
    "#\n",
    "\n",
    "# for completeness, since this might also become a 'mk_FOO' jit-fn-generator\n",
    "def do_clustering_ipts_py(idx, pts, cluster_lists, springs, lr, mindist):\n",
    "    \"\"\" python-ish front-end, with appropriate setup for just internals.\n",
    "\n",
    "        pts: array[n_samples,dim]  sample ~ \"idx\"\n",
    "        clusters: list-of-lists ~ (cluster, idx)\n",
    "        springs: python list of numbers (np array[:]\n",
    "        lr, mindist: python numbers\n",
    "    \"\"\"\n",
    "    idx = int(idx)\n",
    "    lr = float(lr)\n",
    "    mindist = float(mindist)\n",
    "    assert len(pts.shape) == 2\n",
    "    assert len(cluster_lists) == len(springs)\n",
    "    n_samples = pts.shape[0]\n",
    "    assert idx < pts.shape[0]\n",
    "    \n",
    "    #np_cluster_lists = [np.array(clist,np.int32) for clist in cluster_lists]\n",
    "    n_clust = len(cluster_lists)\n",
    "\n",
    "    if n_samples==0 or n_clust==0:\n",
    "        return\n",
    "    \n",
    "    #\n",
    "    # generalization:  clusters[c,idx] is True IFF idx is in cluster c\n",
    "    clusters = np.full((n_clust, n_samples), False, dtype=bool)\n",
    "    for (c,members) in enumerate(cluster_lists):\n",
    "        for m in members:\n",
    "            clusters[c][m] = True\n",
    "            \n",
    "    springs = np.array(springs, dtype=np.float32)  # don't need python float64 default\n",
    "    # no negative or inf or nan springs\n",
    "    assert np.all(springs >= 0.0)  # actually nan is also NOT >= 0 so elision...\n",
    "    assert np.count_nonzero((springs == np.inf) | (springs == np.nan)) == 0\n",
    "    \n",
    "    # pts is to be modified -- do not create a copy!\n",
    "    \n",
    "    # invoke jit fn (or create and return it)\n",
    "    xdo_clustering_ipts( idx, pts, clusters, springs, lr=lr, mindist=mindist )\n",
    "\n",
    "    return\n",
    "\n",
    "# python \"frontend-to-jit\" demo for cell output\n",
    "def do_clustering_pts_py(pts, cluster_lists, springs, lr, mindist):\n",
    "    \"\"\" python-ish front-end, with appropriate setup for just internals.\n",
    "\n",
    "        pts: array[n_samples,dim]  sample ~ \"idx\"\n",
    "        clusters: list-of-lists ~ (cluster, idx)\n",
    "        springs: python list of numbers\n",
    "        lr, mindist: python numbers\n",
    "    \"\"\"\n",
    "    lr = float(lr)\n",
    "    mindist = float(mindist)\n",
    "    assert len(pts.shape) == 2\n",
    "    assert len(cluster_lists) == len(springs)\n",
    "    n_samples = pts.shape[0]\n",
    "    \n",
    "    #np_cluster_lists = [np.array(clist,np.int32) for clist in cluster_lists]\n",
    "    n_clust = len(cluster_lists)\n",
    "    \n",
    "    if n_samples==0 or n_clust==0:\n",
    "        return\n",
    "    \n",
    "    clusters = np.full((n_clust, n_samples), False, dtype=bool)\n",
    "    for (c,members) in enumerate(cluster_lists):\n",
    "        for m in members:\n",
    "            clusters[c][m] = True\n",
    "            \n",
    "    springs = np.array(springs, dtype=np.float32)  # don't need python float64 default\n",
    "    # no negative or inf or nan springs\n",
    "    assert np.all(springs >= 0.0)  # actually nan is also NOT >= 0 so elision...\n",
    "    assert np.count_nonzero((springs == np.inf) | (springs == np.nan)) == 0\n",
    "    \n",
    "    # pts is to be modified -- do not create a copy!\n",
    "    \n",
    "    if False:\n",
    "        print(f\"{numba.typeof(pts)=} {pts.shape=}\")\n",
    "        print(f\"{numba.typeof(clusters)=}\")\n",
    "        print(f\"{numba.typeof(springs)=}\")\n",
    "        print(f\"{numba.typeof(lr)=}\")\n",
    "        print(f\"{numba.typeof(mindist)=}\")\n",
    "    \n",
    "    # invoke jit fn (or create and return it)\n",
    "    xdo_clustering_pts( pts, clusters, springs, lr=lr, mindist=mindist )\n",
    "    \n",
    "    return\n",
    "\n",
    "#\n",
    "# --------------------- Inputs -----------------\n",
    "#\n",
    "\n",
    "n_samples = 8\n",
    "cluster_lists = [[1,2,3], [2,7,6]]\n",
    "#           avg    2.0      5.0\n",
    "# each of the 2 clusters has a spring constant\n",
    "springs = np.array([0.8, 2.0], dtype=np.float32)\n",
    "lr = 1.0   # learning rate (gradient multiplier) for spring forces\n",
    "\n",
    "pts = np.ndarray((n_samples,2), dtype=np.float32)\n",
    "for i in range(n_samples):\n",
    "    pts[i,:] = (float(i),float(i%3))\n",
    "\n",
    "\n",
    "#np_cluster_lists = [np.array(clist,np.int32) for clist in cluster_lists]\n",
    "#n_clust = len(np_cluster_lists)\n",
    "## numba-friendly: single full-sized array\n",
    "##  TODO: compressed data,indptr,indices versions cluster and its transpose\n",
    "#\n",
    "## clusters[ c, idx ], for c in [0,n_clust] and idx in [0,n_samples),\n",
    "##         is True IFF idx is in cluster c\n",
    "#clusters = np.full((n_clust, n_samples), False, dtype=bool)\n",
    "#for (c,members) in enumerate(np_cluster_lists):\n",
    "#    for m in members:\n",
    "#        clusters[c][m] = True\n",
    "#print(f\"{numba.typeof(clusters)=}\")\n",
    "\n",
    "#\n",
    "# ------------------------ python/jit test -------------\n",
    "#\n",
    "mindist=0.2 # support TBD (need to rething equations)\n",
    "\n",
    "#xdo_clustering_pts(pts, clusters, springs, lr=lr, mindist=0.2)\n",
    "do_clustering_pts_py(pts, cluster_lists, springs, lr=lr, mindist=0.2)\n",
    "\n",
    "print(\"final positions (one epoch)\")\n",
    "for i in range(pts.shape[0]):\n",
    "    print(f\"{i=} pt = {pts[i,:]}\")\n",
    "\n",
    "try:\n",
    "    pts_ref\n",
    "    assert np.allclose(pts, pts_ref)\n",
    "    print(\"Good: matched pts_ref\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(\"Goodbye!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FINALLY have some *mk_FOO*\n",
    "### jitted cluster-funcs\n",
    "\n",
    "#### What's the point?\n",
    "*umap-constraints, short-n-sweet* jitted \"lambda functions\" can be used\n",
    "as `data_constrain=` or `output_constrain=` args to umap euclidean\n",
    "embedding calls (`fit`, `fit_transform`)\n",
    "\n",
    "The umap embedding should *pull together* our **user-specified clusters**,\n",
    "with whatever effects these have on next-nearest members, etc. ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now try above cell as part of umap-constraints...\n",
    "# This call can run standalone:\n",
    "#   It invokes a jit-function mk_FOO,\n",
    "#   and then invokes it\n",
    "#   (umap.UMAP not involved)\n",
    "#\n",
    "#   I found the mk_FOO marks some local array refs 'readonly array'.\n",
    "#   It was easiest to simply remove the overly constraining specs\n",
    "#   and let @numba.njit() autogenerate the required signatures\n",
    "#   in umap/constrain_clust.py\n",
    "#\n",
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "from umap.constrain_clust import mk_clustering_pts, mk_clustering_ipts\n",
    "\n",
    "#\n",
    "# ------- Inputs (python) -----------------------------\n",
    "n_samples = 8\n",
    "cluster_lists = [[1,2,3], [2,7,6]]\n",
    "#           avg    2.0      5.0\n",
    "# each of the 2 clusters has a spring constant\n",
    "springs = np.array([0.8, 2.0], dtype=np.float32)\n",
    "lr = 1.0   # learning rate (gradient multiplier) for spring forces\n",
    "\n",
    "pts = np.ndarray((n_samples,2), dtype=np.float32)\n",
    "for i in range(n_samples):\n",
    "    pts[i,:] = (float(i),float(i%3))\n",
    "\n",
    "mindist = 0.2\n",
    "\n",
    "#\n",
    "# ------------ create & call jit constraint -----------\n",
    "# Now that we have the python-ish inputs set up,\n",
    "# create a jit constraint function of simplified signature\n",
    "#\n",
    "mkdo_pts = mk_clustering_pts(pts, cluster_lists, springs, lr=lr, mindist=mindist)\n",
    "\n",
    "# and just invoke it as\n",
    "mkdo_pts(pts)  # simplified call signature, other args are now mk_FOO locals\n",
    "#    without re-supplying all the args like\n",
    "#        do_clustering_pts_py(pts, cluster_lists, springs, lr=lr, mindist=0.2)\n",
    "\n",
    "#\n",
    "# ------------- output --------------------------------\n",
    "# check we got the same \"output\" (in-place modification of our pts array)\n",
    "#\n",
    "print(\"final positions (one epoch)\")\n",
    "for i in range(pts.shape[0]):\n",
    "    print(f\"{i=} pt = {pts[i,:]}\")\n",
    "\n",
    "try:\n",
    "    pts_ref\n",
    "    assert np.allclose(pts, pts_ref)\n",
    "    print(\"Good: matched pts_ref\")\n",
    "except NameError:\n",
    "    pass\n",
    "\n",
    "print(\"Goodbye!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numba\n",
    "\n",
    "from umap.constrain_clust import mk_clustering_pts, mk_clustering_ipts\n",
    "\n",
    "\n",
    "#\n",
    "# WIP:  This cell should use mk_FOO clustering for the iris data set,\n",
    "#       actually running a umap embedding showing user-clustering via\n",
    "#       the \"additional springs\" approach.\n",
    "# Based on plots/ behaviors,\n",
    "# I expect the mk_FOO calls may have some behavioral modifiers\n",
    "# like:\n",
    "#    - actually supporting the mindist \"target radius\" clustering arg\n",
    "#    - weakening connections to next-nearest-neighbors (alpha?)\n",
    "#    - ...\n",
    "#\n",
    "cluster_lists = [[0, 50, 100, 149]]  # a single cluster\n",
    "print(\"# let's cluster pts\", cluster_lists, \"with a big force\")\n",
    "for idx in cluster_lists[0]:\n",
    "    print(f\"  cluster {idx=} {emb0[idx]=}\")\n",
    "cl_pts = emb0[cluster_lists[0],:]\n",
    "cl_avg = np.mean(cl_pts, axis=0)\n",
    "cl_avg2 = umap.constrain_clust.np_mean_axis_0(cl_pts)\n",
    "print(f\" emb0 cluster centroid  @ {cl_avg}\")\n",
    "print(f\" emb0 cluster centroid' @ {cl_avg2}\")\n",
    "\n",
    "# set things so cluster pts move \"all the way\" to their target\n",
    "springs = [1.0]\n",
    "lr = 1.0\n",
    "mindist = 0.0\n",
    "\n",
    "print(\"\\nmove individual pts via python do_clustering_ipts_py\")\n",
    "t = emb0.copy()\n",
    "for idx in [0, 1]:\n",
    "    do_clustering_ipts_py(idx, t, cluster_lists, springs, lr, mindist)\n",
    "    print(f\" cluster {idx=} ({emb0[idx,0]:.2f},{emb0[idx,1]:.2f}) to ({t[idx,0]:.2f},{t[idx,1]:.2f})\")\n",
    "\n",
    "print(\"\\nmove all pts in cluster via python do_clustering_pts_py\")\n",
    "t = emb0.copy()\n",
    "for idx in cluster_lists[0]:\n",
    "    do_clustering_ipts_py(idx, t, cluster_lists, springs, lr, mindist)\n",
    "    print(f\" cluster {idx=} ({emb0[idx,0]:.2f},{emb0[idx,1]:.2f}) to ({t[idx,0]:.2f},{t[idx,1]:.2f})\")\n",
    "\n",
    "print(\"\\nmove all pts via mk_clustering_pts constraint-generator\")\n",
    "all_points_clusterer = mk_clustering_pts(\n",
    "    iris.data, # for nsamples=shape[0], we'll actually call using lo-D embedding features\n",
    "    cluster_lists,\n",
    "    springs,\n",
    "    lr=1.0, # timestep: enough for spring 1.0 to get all the way\n",
    "    mindist=0.0,   # really go all the way\n",
    ")\n",
    "t = emb0.copy()\n",
    "all_points_clusterer(t)\n",
    "for idx in cluster_lists[0]:\n",
    "    print(f\" cluster {idx=} ({emb0[idx,0]:.2f},{emb0[idx,1]:.2f}) to ({t[idx,0]:.2f},{t[idx,1]:.2f})\")\n",
    "print(\"  note that after 1st pt moved, cluster center changed\")\n",
    "print(\"  so pts 2,3,4 in cluster moved to slightly different locations\")\n",
    "print(\"  trying another 2 'epochs' to converge better (mindist is {mindist})\")\n",
    "for i in range(2):\n",
    "    all_points_clusterer(t)\n",
    "for idx in cluster_lists[0]:\n",
    "    print(f\" cluster {idx=} ({emb0[idx,0]:.2f},{emb0[idx,1]:.2f}) to ({t[idx,0]:.2f},{t[idx,1]:.2f})\")\n",
    "\n",
    "print(\"\\nmove individual pts via mk_clustering_ipts constraint-generator\")\n",
    "point_clusterer = mk_clustering_ipts(\n",
    "    0,   # idx, unused\n",
    "    iris.data, # for nsamples=shape[0], we'll actually call using lo-D embedding features\n",
    "    cluster_lists,\n",
    "    springs,\n",
    "    lr=1.0, # timestep: enough for spring 1.0 to get all the way\n",
    "    mindist=0.0,\n",
    ")\n",
    "t = emb0.copy()\n",
    "point_clusterer(0, t)\n",
    "point_clusterer(1, t)\n",
    "for idx in [0,1]:\n",
    "    point_clusterer(idx, t)\n",
    "    print(f\" cluster {idx=} ({emb0[idx,0]:.2f},{emb0[idx,1]:.2f}) to ({t[idx,0]:.2f},{t[idx,1]:.2f})\")\n",
    "\n",
    "print(\"Goodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # If I had only the clustering constraint(idx,pt), ...\n",
    "    constraints = {\n",
    "        'idx_ipts': point_clusterer\n",
    "    }\n",
    "else:\n",
    "    #\n",
    "    # ... but I have not supported LISTS of CONSTRAINTS, but I can package\n",
    "    #     the point clusterer with another constraint like this:\n",
    "    #\n",
    "    #  (you can use print in numba, but only strings and values, no kwargs either)\n",
    "    #\n",
    "    @numba.njit(\"f4[:](i8, f4[:,:])\")\n",
    "    def constraint_idx_pt1(idx,pts):\n",
    "        # Here 'pts' MUST be 2D (full point cloud)\n",
    "        #pt0 = pts[idx,:].copy()\n",
    "        point_clusterer(idx, pts)\n",
    "        #if idx==0:\n",
    "        #    print(\" idx0 (\",pt0[0],\",\",pt0[1],\") to (\",pts[idx,0],\",\",pts[idx,1],\")\")\n",
    "        return con.freeinf_ipts(idx,pts, infs0)\n",
    "    # this function DOES depent on idx of pt\n",
    "\n",
    "    constraints = {\n",
    "        'idx_ipts': constraint_idx_pt1,  # NEW _ipts suffix ==> alt call signature\n",
    "        # actually this is most flexible since the full \"tail_embedding\"\n",
    "        # point cloud is actually always available.\n",
    "    }\n",
    "\n",
    "print(\"# let's begin with 11:15 an a square around the origin\")\n",
    "print(\"# their cluster will still feel pulls of neighbors\")\n",
    "emb0[11] = [+3.14, +3.14]\n",
    "emb0[12] = [+3.14, -3.14]\n",
    "emb0[13] = [-3.14, +3.14] # init with far away coords\n",
    "emb0[14] = [+3.14, -3.14]\n",
    "# optional: set up the values to agree\n",
    "#emb0[13] = [-2.0, 0]\n",
    "#emb0[14] = [+2.0, 0]\n",
    "#assert np.all(emb0[13] == [-2.0,0])\n",
    "#assert np.all(emb0[14] == [+2.0,0])\n",
    "# Here is the \"move all points\" version of con.freeinf\n",
    "con.freeinf_pts(emb0, infs0)\n",
    "\n",
    "# Also demo a non-indexed (UMAP constructor) constraint,\n",
    "# that is independent of the iris.data.\n",
    "# Here, let's constrain 'y' to be within -5.0, +5.0\n",
    "# without, we got:\n",
    "# emb2[11:15]\n",
    "#  [[ 4.804111   3.430179 ]\n",
    "#  [ 4.2598176  7.352637 ]     # <-- 'y' is big here\n",
    "#  [-2.         0.       ]\n",
    "#  [ 2.         0.       ]]\n",
    "# With 'output_constrain':\n",
    "# emb2[11:15]\n",
    "# [[ 7.0536     2.275853 ]\n",
    "# [ 4.8699265  1.9713866]      # y constrained\n",
    "# [-2.         0.       ]\n",
    "# [ 2.         0.       ]]\n",
    "# So we pass illegal range for x, and legal range for y\n",
    "def mk_bound_y_values(lo, hi):\n",
    "    bound_los = np.array([+999.,lo], dtype=np.float32)\n",
    "    bound_his = np.array([-999.,hi], dtype=np.float32)\n",
    "    @numba.njit()\n",
    "    def bound_y_values(pt):\n",
    "        return con.dimlohi_pt(pt, bound_los, bound_his)\n",
    "    # this function does NOT depend on 'idx' arg\n",
    "    return bound_y_values\n",
    "y_bounder = mk_bound_y_values(-5.0,+5.0)\n",
    "\n",
    "# CHECK: x is unaffected, y range is bounded ...\n",
    "pt = np.array([1.,2.], dtype=np.float32)\n",
    "print(\"pt0\",pt); y_bounder(pt); print(\"pt0\",pt); assert pt[0] == 1.; assert pt[1] == 2.\n",
    "pt[0] = 10.; pt[1] = 10.\n",
    "print(\"pt1\",pt); y_bounder(pt); print(\"pt1\",pt); assert pt[0] == 10.; assert pt[1] == 5.\n",
    "pt[0] = -10.; pt[1] = -10.\n",
    "print(\"pt2\",pt); y_bounder(pt); print(\"pt2\",pt); assert pt[0] == -10.; assert pt[1] == -5.\n",
    "\n",
    "print(\"Specify 'init' embedding for umapper2\")\n",
    "umapper6 = umap.UMAP(\n",
    "    n_neighbors=50, learning_rate=0.5, random_state=12346, min_dist=0.001,\n",
    "    output_constrain = { 'pt': y_bounder }, # any pt, ind't of dataset\n",
    "    init=emb0, n_epochs=4,\n",
    ")\n",
    "print(\"Embed with pin_mask[13] and pin_mask[14] zero-vectors\")\n",
    "# ... whereas data_constrain depends on the dataset (point number is important)\n",
    "emb6 = umapper6.fit_transform(iris.data, data_constrain=constraints)\n",
    "assert np.all(emb6[:,1] >= -5.0) and np.all(emb6[:,1] <= 5.0) # output_constrain\n",
    "print(\"emb0[11:15]\\n\",emb0[11:15])\n",
    "print(\"emb6[11:15]\\n\",emb6[11:15])\n",
    "assert np.allclose(emb6[13], [-2.2,0])\n",
    "assert np.allclose(emb6[14], [+2.2,0])\n",
    "\n",
    "print(\"\\nWhat happened to cluster-constrained umap pts?\")\n",
    "for idx in cluster_lists[0]:\n",
    "    print(f\" cluster {idx=} ({emb0[idx,0]:.2f},{emb0[idx,1]:.2f}) to ({emb6[idx,0]:.2f},{emb6[idx,1]:.2f})\")\n",
    "print(\"After printing the right thing, the cluster really does exist!\")\n",
    "\n",
    "print(\"\\nGoodbye\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:miru] *",
   "language": "python",
   "name": "conda-env-miru-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
